# Training Configuration for LoRA Fine-tuning with Flux

# Model settings
model:
  name: "flux"
  pretrained_model_name_or_path: "black-forest-labs/FLUX.1-dev"
  revision: "main"
  
  # LoRA settings
  lora:
    rank: 16
    alpha: 32
    dropout: 0.1
    target_modules:
      - "to_k"
      - "to_q"
      - "to_v"
      - "to_out.0"
    bias: "none"

# Training settings
training:
  output_dir: "output"
  logging_dir: "logs"
  
  # Training parameters
  num_train_epochs: 100
  max_train_steps: 2000
  train_batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 1e-4
  lr_scheduler: "cosine"
  lr_warmup_steps: 100
  lr_num_cycles: 1
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  
  # Mixed precision training
  mixed_precision: "fp16"
  
  # Checkpointing
  checkpointing_steps: 1000  # Increased to reduce frequency of checkpoint creation
  checkpoints_total_limit: 2  # Reduced to limit disk usage and memory pressure
  
  # Validation
  validation_steps: 100
  num_validation_images: 4
  
  # Memory optimization
  gradient_checkpointing: true
  enable_xformers_memory_efficient_attention: true
  use_8bit_adam: false
  
  # Hardware settings
  device: "cuda"
  num_gpu: 1
  seed: 42

# Data settings
data:
  dataset_config: "configs/dataset_config.yaml"
  dataloader_num_workers: 4
  prefetch_factor: 2

# Optimization for RTX 1080 (8GB VRAM)
optimization:
  # Reduce memory usage
  train_text_encoder: false
  text_encoder_lr: 1e-5
  vae_lr: 1e-5
  unet_lr: 1e-4
  
  # Batch size optimization
  train_batch_size: 1
  gradient_accumulation_steps: 4
  
  # Precision settings
  mixed_precision: "fp16"
  
  # Memory saving techniques
  gradient_checkpointing: true
  enable_xformers_memory_efficient_attention: true
  
  # CPU offloading if needed
  cpu_offloading: true  # Enabled to reduce GPU memory pressure
  
  # Attention slicing for memory efficiency
  attention_slicing: true
  
  # Additional memory optimization for checkpoint creation
  checkpoint_memory_optimization: true
  force_checkpoint_cpu_offload: true

# Logging and monitoring
logging:
  use_wandb: false
  wandb_project: "flux-lora-finetuning"
  wandb_run_name: "flux-lora"
  
  use_tensorboard: true
  log_steps: 10
  
  save_model_steps: 500
  save_images_steps: 100